主要步骤与流程：
1、预处理，去除文本的噪声信息
2、中文分词：使用中文分词器为文本分词，去除停用词
3、构建词向量空间：统计文本词频、生成文本的词向量空间
4、权重策略------TF-IDF方法：使用TF-IDF发现特征词，并抽取为反映文档主题的特征
5、分类器：使用算法训练分类器
6、评价分类结构：分类器的测试结果分析


文本预处理：主要是结构化数据
    1、选择处理的文本的范围,适当的范围取决于文本挖掘任务的目标，对于分类或聚类的任务往往将整个文档作为处理单位，对于情感分析、文档懂文摘、信息检索，通常段落或章节更合适
    2、建立分类文本语料库：
        1）训练集语料  http://www.threedweb.cn/thread-1288-1-1.html
        2）测试集语料
    3、文本格式转换
    4、检测句子边界：标记句子的结束


中文分词：小巧高效支持原生python的分词系统 jieba分词。
    1、向量空间模型，通过向量的每个特征表示文本中出现的词，（停用词的处理）
    2、权重策略：TF-IDF方法(词频逆文档频率)
        1）增加词频信息就变成了整型计数方式，可以进行归一化，避免了句子长度不一致的问题，便于算法计算，同时词频信息可以映射到概率分布，这就是文档的TF信息
        2）词袋的统计基数是文档数
        3）逆文档频率就是使用词条的文档频率抵消盖茨的词频对权重的影响从而得到一个较低的权重
    3、使用朴素贝叶斯分类模块
        1）常用文本分类方法：kNN最近邻算法、朴素贝叶斯算法、支持向量机算法
    4、分类结果评估
        1）召回率（查全率）衡量的是检索系统的查全率
        2）准确率（精度）衡量的时检索系统的查准率
        3）F-Sorce
    5、分类算法：朴素贝叶斯
        1）朴素贝叶斯分类流程：
            1】训练数据生成训练样本集：TF-IDF
            2】对每个类别计算P(y)
            3】对每个特征属性计算所有划分的条件概率
            4】对每个类别计算
            5】以最大项作为x的所属类别



